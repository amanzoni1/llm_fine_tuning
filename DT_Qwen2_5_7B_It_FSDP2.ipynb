{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Distributed Training - Qwen2.5-7B-Instruct with FSDP2\n\nThis Kaggle notebook demonstrates fine-tuning the Qwen2.5-7B-Instruct model using Fully Sharded Data Parallelism version 2 (FSDP2) with QLoRA (4-bit quantization via BitsAndBytes) for parameter-efficient training. The setup runs on Kaggle's free dual T4 GPUs, leveraging mixed precision, CPU offloading, activation checkpointing, and LoRA adapters to minimize VRAM usage during distributed training.\n\nThe training logic is in a Python script (required for Accelerate’s multi-process launch) with inline comments for clarity. Post-training, we plot the training loss to visualize progress.\n\nThis aligns with the repo’s focus on efficient, GPU-friendly fine-tuning.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Setup","metadata":{}},{"cell_type":"code","source":"!pip install --quiet trl accelerate==1.0.1 peft bitsandbytes git+https://github.com/huggingface/transformers.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Accelerate FSDP2 Configuration\nThe YAML config enables FSDP2 with:\n\n\n\n\n\n- Sharding (version 2) and auto-wrapping for transformer layers.\n\n- Optimizations: Reshard after forward, activation checkpointing, CPU offload, FP16 mixed precision.\n\n\n\n- Full state dict for saving.\n\nThese reduce per-GPU memory usage for the 7B model.","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/fsdp2_config.yaml\n\ncompute_environment: LOCAL_MACHINE\ndistributed_type: FSDP\ndowncast_bf16: no\nfsdp_config:\n  fsdp_version: 2\n  reshard_after_forward: true\n  transformer_cls_names_to_wrap: Qwen2DecoderLayer\n  auto_wrap_policy: TRANSFORMER_BASED_WRAP  \n  state_dict_type: FULL_STATE_DICT\n  activation_checkpointing: true\n  cpu_offload: true\n  limit_all_gathers: true\n  mixed_precision_policy:\n    param_dtype: float16\n    reduce_dtype: float32\n    output_dtype: float16\n    cast_forward_inputs: true\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training Script","metadata":{}},{"cell_type":"code","source":"%%writefile fsdp2_train.py\n\nimport os, gc, torch, json\nfrom torch.distributed import destroy_process_group\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainerCallback\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom peft.utils.other import fsdp_auto_wrap_policy\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import load_dataset\nfrom huggingface_hub import login\n\n# Define training hyperparameters as a dict for easy modification\nTRAIN_CONFIG = {\n    \"seed\": 123,  # For reproducibility\n    \"max_steps\": 100,  # Limited number of steps for demo\n    \"warmup_steps\": 5,  # Quick warmup to stabilize LR\n    \"batch_size\": 1,   # Per-device batch size\n    \"grad_accum_steps\": 8,  # Accumulate gradients for effective larger batch\n    \"max_seq_length\": 1024,  # Balance context vs. memor\n    \"learning_rate\": 5e-5,  # Conservative LR for stable training\n    \"logging_steps\": 5,  # Frequent logging for monitoring\n    \"output_dir\": \"/kaggle/working/output\",\n}\n\n# Environment variables for optimization\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"  # Faster HF downloads\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Use both GPUs\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"  # Optimize CUDA memory allocation\n\n# Custom callback to log GPU memory usage and save training logs\nclass GPUUsageCallback(TrainerCallback):\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if state.is_local_process_zero:  # Only save on rank 0 to avoid duplicates\n            mem_alloc = torch.cuda.memory_allocated() / 1e9\n            mem_reserved = torch.cuda.memory_reserved() / 1e9\n            mem_peak = torch.cuda.max_memory_allocated() / 1e9\n            print(f\"[Mem Alloc = {mem_alloc:.2f}GB, Mem Reserved = {mem_reserved:.2f}GB, Mem Peak = {mem_peak:.2f}GB]\")\n            \n            # Save step and loss to JSON\n            logs_to_save = {\"step\": state.global_step, \"loss\": logs.get(\"loss\", None), \"mem_alloc\": mem_alloc, \"mem_reserved\": mem_reserved}\n            with open(os.path.join(args.output_dir, \"train_logs.json\"), \"a\") as f:\n                f.write(json.dumps(logs_to_save) + \"\\n\")\n                \n        return control\n\n# Function to load dataset, model, and tokenizer\ndef load_train_objs():\n    local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n    if local_rank != -1:\n        torch.cuda.set_device(local_rank)  # Set device per process\n\n    model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n\n    # Load tokenizer and set pad token\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    # 4-bit quantization config for QLoRA\n    bnb_cfg = BitsAndBytesConfig(\n        load_in_4bit              = True,\n        bnb_4bit_use_double_quant = True,  # Nested quantization for extra savings\n        bnb_4bit_quant_type       = \"nf4\",  # Normalized float4 for better precision\n        bnb_4bit_compute_dtype    = torch.float16,  # FP16 for computations\n        bnb_4bit_quant_storage    = torch.float16,  # Store quantized weights in FP16\n    )\n\n    # Load model with quantization and device mapping\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        dtype               = torch.float16,  # Overall dtype\n        device_map          = {\"\": local_rank} if local_rank != -1 else \"auto\",  # Map to process device\n        attn_implementation = \"sdpa\",  # Scaled dot-product attention for efficiency\n        low_cpu_mem_usage   = True,  # Reduce CPU memory during loading\n        quantization_config = bnb_cfg,\n    )\n\n    # Load dataset (10% for demo)\n    url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n    dataset = load_dataset(\"json\", data_files={\"train\": url}, split=\"train[:10%]\")\n\n    return dataset, model, tokenizer\n\n# Function to apply LoRA adapters\ndef apply_lora(model):\n    # LoRA config: Target key modules, low dropout, causal LM task\n    lora_cfg = LoraConfig(\n        r              = 16,  # Rank of adapters\n        lora_alpha     = 32,  # Scaling factor\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n        lora_dropout   = 0.0,\n        bias           = \"none\",\n        task_type      = TaskType.CAUSAL_LM,\n    )\n\n    # Get LoRA and setup model\n    model = get_peft_model(model, lora_cfg)\n\n    # Freeze base model, only train LoRA params\n    with torch.no_grad():\n        for name, param in model.named_parameters():\n            if \".lora_A.\" in name or \".lora_B.\" in name:\n                param.requires_grad_(True)\n            else:\n                param.requires_grad_(False)\n\n    model.enable_input_require_grads()  # For gradient checkpointing if enabled\n    model.config.use_cache = False  # Disable cache during training\n\n    return model\n\n# Main training function\ndef main():\n    # Load objects\n    dataset, model, tokenizer = load_train_objs()\n    model = apply_lora(model)\n\n    # Clear cache before training\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # SFTTrainer args with FSDP integration\n    training_args = SFTConfig(\n        seed                        = TRAIN_CONFIG[\"seed\"],\n        max_steps                   = TRAIN_CONFIG[\"max_steps\"],\n        warmup_steps                = TRAIN_CONFIG[\"warmup_steps\"],\n        per_device_train_batch_size = TRAIN_CONFIG[\"batch_size\"],\n        gradient_accumulation_steps = TRAIN_CONFIG[\"grad_accum_steps\"],\n        max_length                  = TRAIN_CONFIG[\"max_seq_length\"],\n        learning_rate               = TRAIN_CONFIG[\"learning_rate\"],\n        logging_steps               = TRAIN_CONFIG[\"logging_steps\"],\n        output_dir                  = TRAIN_CONFIG[\"output_dir\"],\n        fp16                        = model.get_input_embeddings().weight.dtype == torch.float16,\n        bf16                        = model.get_input_embeddings().weight.dtype == torch.bfloat16,\n        fsdp                        = \"full_shard\",  # Full sharding for FSDP\n        gradient_checkpointing      = False,  # Disabled here; enabled in FSDP2 config\n        report_to                   = \"none\",  # No external logging\n    )\n\n    # Initialize trainer\n    trainer = SFTTrainer(\n        model            = model,\n        args             = training_args,\n        train_dataset    = dataset,\n        processing_class = tokenizer,\n        callbacks        = [GPUUsageCallback()],  # Memory logging\n    )\n\n    # Print FSDP setup info\n    rank = trainer.accelerator.process_index\n    print(f\"[Rank {rank}] Plugin FSDP version: {trainer.accelerator.state.fsdp_plugin.fsdp_version}\")\n    print(f\"[Rank {rank}] is_fsdp2: {trainer.accelerator.is_fsdp2}\")\n    print(f\"[Rank {rank}] Distributed type: {trainer.accelerator.state.distributed_type}\")\n    print(f\"[Rank {rank}] Device: {trainer.accelerator.device}\")\n\n    # Pre-training memory check\n    rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n    mem_alloc = torch.cuda.memory_allocated() / 1e9\n    mem_reserved = torch.cuda.memory_reserved() / 1e9\n    print(f\"[Rank {rank}] Pre-training Mem Alloc = {mem_alloc:.2f}GB, Mem Reserved = {mem_reserved:.2f}GB\")\n\n    # Print trainable params\n    trainer.model.print_trainable_parameters()\n\n    # Start training\n    trainer.train()\n\n    # Save model with full state dict\n    if trainer.is_fsdp_enabled:\n        trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n    trainer.save_model()\n\n    # Clean up distributed processes\n    destroy_process_group()\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Launch Training\n\nLaunch with Accelerate using 2 GPUs. Monitor memory usage and training progress","metadata":{}},{"cell_type":"code","source":"!accelerate launch --config_file fsdp2_config.yaml --num_processes 2 fsdp2_train.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training Loss Visualization\n\nSince no evaluation dataset is used, we plot the training loss from the saved logs. The GPUUsageCallback saves logs to train_logs.json, which we load and visualize.","metadata":{}},{"cell_type":"code","source":"import json\nimport matplotlib.pyplot as plt\nfrom seaborn import sns\n\n# Load training logs\nlog_file = \"/kaggle/working/output/train_logs.json\"\ntrain_entries = []\nwith open(log_file, \"r\") as f:\n    for line in f:\n        log = json.loads(line)\n        if log.get(\"loss\") is not None:\n            train_entries.append((log[\"step\"], log[\"loss\"]))\n\n# Extract steps and losses\ntrain_steps, train_losses = zip(*train_entries) if train_entries else ([], [])\n\n# Plot\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"pastel\")\nplt.figure(figsize=(8, 4))\nplt.plot(train_steps, train_losses, marker='o', label='Training Loss')\nplt.xlabel(\"Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss over Steps\")\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}